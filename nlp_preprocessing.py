# -*- coding: utf-8 -*-
"""NLP Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TuN2wl_WJaKJQbrM77qI86FShN0fETvY

## NLP Assignment

# Installing and Importing Libraries
"""

!pip uninstall -y numpy
!pip install numpy==1.23.5

#Restart Runtime after this is done
# Just run it once, then disconnect and delete runtime, then skip this cell, you don't need it anymore

!pip uninstall -y pandas
!pip install pandas==1.5.3 

!pip uninstall -y scikit-learn
!pip install scikit-learn==1.2.0

!pip uninstall -y nltk
!pip install nltk==3.8.1

!pip uninstall -y torch
!pip install torch==1.13.1

# Just run it once, then disconnect and delete runtime, then skip this cell, you don't need it anymore

!pip install pytorch-lightning==1.8.1
!pip install transformers==4.22.2
!pip install datasets==2.9.0 
!pip install sentencepiece==0.1.97
!pip install stanza==1.4.2

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re, string
import nltk.data
import nltk
from matplotlib import pyplot as plt
# %matplotlib inline

# Download English 
nltk.download('stopwords')
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

pd.set_option('max_colwidth', None)

"""# Prepare Train Data"""

data = pd.read_csv('traindata.csv',sep='	',header=None)

data.head(3)

#renaming the columns 
data = data.rename(columns={0:'Sentiment',1:'Category',2:'Subject',3:'Index',4:'Text'})

data.head(3)

def process_string(text):
  # 1. remove tags
  text = re.sub(re.compile('<.*?>'), ' ', text)
  # 2. remove punctuation and lowercase
  text = re.sub(r'[^\w\s\']',' ', text).lower()
  # 3. remove duplicate spaces
  text = re.sub(' +', ' ', text)
  return text

# before
print('Before preprocessing:')
print(data["Text"][112])

# preprocess
data["Text"] = data["Text"].apply(process_string)

# after
print("Example of a preprocessed text:")
print(data["Text"][112])

data.Category.unique()

# Separating 2 columns
data[['Category','Sub_Category']] = data['Category'].str.split('#', 1, expand=True)

data.head(3)

# Examine type of each column
data.dtypes

# Checking NA values
data.isna().sum()

# Label Encoding Category and Sub-Category
data['Category'] = data['Category'].apply(lambda x: 1 if x == 'AMBIENCE' else (2 if x == 'FOOD' else (3 if x == 'SERVICE' else (4 if x == 'RESTAURANT' else (5 if x == 'DRINKS' else 6)))))
data['Sub_Category'] = data['Sub_Category'].apply(lambda x: 1 if x == 'GENERAL' else (2 if x == 'QUALITY' else (3 if x == 'STYLE_OPTIONS' else (4 if x == 'MISCELLANEOUS' else 5))))

data.head(3)

# pre-process Subject column
data["Subject"] = data["Subject"].apply(process_string)

# Remove the stopwords based on whether its an important word or not
stopwords_ENG = stopwords.words('english')

# Turn into a set
stop_words = set(stopwords_ENG)

def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word.lower() not in stop_words]
    return " ".join(words)

# Examine type of each column
data.dtypes

# Remove stopwords from Text and Subject
data['Text'] = data['Text'].apply(remove_stopwords)
data['Subject'] = data['Subject'].apply(remove_stopwords)

# Changing Index column
data['Index'] = data.apply(lambda x: str(x['Text'].find(x['Subject']))+'#'+str(x['Text'].find(x['Subject'])+ len(x['Subject'])), axis = 1)

# Checking NA values
data.isna().sum()

data.head(3)

# Split index left and right
data[['Index_Left','Index_Right']] = data['Index'].str.split('#', 1, expand=True)

data.drop('Index', axis = 1, inplace = True)
data['Index_Left'] = data['Index_Left'].astype(int)
data['Index_Right'] = data['Index_Right'].astype(int)

data.head(3)

# Label Encode Sentiment
le = LabelEncoder()
data['Sentiment'] = le.fit_transform(data['Sentiment'])

"""# Prepare Test Data"""

test = pd.read_csv('devdata.csv',sep='	',header=None)

test.head()

#renaming the columns 
test = test.rename(columns={0:'Sentiment',1:'Category',2:'Subject',3:'Index',4:'Text'})

# before
print('Before preprocessing:')
print(test["Text"][112])

# preprocess
test["Text"] = test["Text"].apply(process_string)

# after
print("Example of a preprocessed text:")
print(test["Text"][112])

test.Category.unique()

# Separating 2 columns
test[['Category','Sub_Category']] = test['Category'].str.split('#', 1, expand=True)

# Checking NA values
test.isna().sum()

# Label Encoding Category and Sub-Category
test['Category'] = test['Category'].apply(lambda x: 1 if x == 'AMBIENCE' else (2 if x == 'FOOD' else (3 if x == 'SERVICE' else (4 if x == 'RESTAURANT' else (5 if x == 'DRINKS' else 6)))))
test['Sub_Category'] = test['Sub_Category'].apply(lambda x: 1 if x == 'GENERAL' else (2 if x == 'QUALITY' else (3 if x == 'STYLE_OPTIONS' else (4 if x == 'MISCELLANEOUS' else 5))))

# pre-process Subject column
test["Subject"] = test["Subject"].apply(process_string)

# Remove stopwords from Text and Subject
test['Text'] = test['Text'].apply(remove_stopwords)
test['Subject'] = test['Subject'].apply(remove_stopwords)

# Changing Index column
test['Index'] = test.apply(lambda x: str(x['Text'].find(x['Subject']))+'#'+str(x['Text'].find(x['Subject'])+ len(x['Subject'])), axis = 1)

# Split index left and right
test[['Index_Left','Index_Right']] = test['Index'].str.split('#', 1, expand=True)

test.drop('Index', axis = 1, inplace = True)
test['Index_Left'] = test['Index_Left'].astype(int)
test['Index_Right'] = test['Index_Right'].astype(int)

# Label Encode Sentiment
le = LabelEncoder()
test['Sentiment'] = le.fit_transform(test['Sentiment'])

test.head()

"""# Method 1: bag of words"""

# Train
count_vect = CountVectorizer(stop_words='english', min_df=5)
X_train_counts = count_vect.fit_transform(data["Text"])
print("Train shape:", X_train_counts.shape)

# Test
X_test_counts = count_vect.transform(test["Text"])
print("Test shape:", X_test_counts.shape)

# Train
feature_names = count_vect.get_feature_names_out()
X_train_vec = pd.DataFrame(X_train_counts.toarray(), columns = feature_names)
df_train = data.drop('Text', axis = 1).merge(X_train_vec, left_index=True, right_index=True)

# Test
feature_names = count_vect.get_feature_names_out()
X_test_vec = pd.DataFrame(X_test_counts.toarray(), columns = feature_names)
df_test = test.drop('Text', axis = 1).merge(X_test_vec, left_index=True, right_index=True)

# Label Encode Subject
le = LabelEncoder()
df_train['Subject'] = le.fit_transform(df_train['Subject'])
df_test['Subject'] = le.fit_transform(df_test['Subject'])

# -- we maybe need to work on classifier.py and tester.py
# split the dataset into training and testing sets
y_train = df_train['Sentiment']
X_train = df_train.drop('Sentiment', axis = 1)
y_test = df_test['Sentiment']
X_test = df_test.drop('Sentiment', axis = 1)

# train a logistic regression model
model = LogisticRegression(max_iter = 10000)
model.fit(X_train, y_train)

# evaluate the performance of the model on the testing data
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, zero_division = True))

"""# Method 2: Word Embedding"""

# a smaller model, faster download and no problems with out of memory
import gensim.downloader as api
wv = api.load('glove-wiki-gigaword-50')

def text_to_vector(Text, wv):
    vectors = []
    for word in Text.split():
        if word in wv: # skip our-of-vocabulary words
            vectors.append(wv[word])
    return np.mean(vectors, axis=0)

X_train_wv = np.zeros((data.shape[0], wv.vectors.shape[1]))
for i, text in enumerate(data["Text"]):
    X_train_wv[i] = text_to_vector(text, wv)

X_test_wv = np.zeros((test.shape[0], wv.vectors.shape[1]))
for i, text in enumerate(test["Text"]):
    X_test_wv[i] = text_to_vector(text, wv)

scaler = StandardScaler()
X_train_wv = scaler.fit_transform(X_train_wv)
X_test_wv = scaler.transform(X_test_wv)

X_train_emb = pd.DataFrame(X_train_wv)
df_train = data.drop('Text', axis = 1).merge(X_train_emb, left_index=True, right_index=True)

X_test_emb = pd.DataFrame(X_test_wv)
df_test = test.drop('Text', axis = 1).merge(X_test_emb, left_index=True, right_index=True)

# All column names as string
df_train.columns = df_train.columns.astype(str)
df_test.columns = df_test.columns.astype(str)

# Label Encode Subject
le = LabelEncoder()
df_train['Subject'] = le.fit_transform(df_train['Subject'])
df_test['Subject'] = le.fit_transform(df_test['Subject'])

# split the dataset into training and testing sets
y_train = df_train['Sentiment']
X_train = df_train.drop('Sentiment', axis = 1)
y_test = df_test['Sentiment']
X_test = df_test.drop('Sentiment', axis = 1)

# train a logistic regression model
model = LogisticRegression(max_iter = 10000)
model.fit(X_train, y_train)

# evaluate the performance of the model on the testing data
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, zero_division = True))

"""# Method 3: Recurrent Neural Network with Tokenization"""

from transformers import GPT2Tokenizer 
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

X_train_tok = []
for text in data["Text"]:
    X_train_tok.append(tokenizer.encode(text))

X_test_tok = []
for text in test["Text"]:
    X_test_tok.append(tokenizer.encode(text))

print(X_train_tok[:3])

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

_ = plt.hist([len(s) for s in X_train_tok], 100)
plt.xlabel("Length")
plt.ylabel("Number of sequences")

X_train_tok = pd.DataFrame(X_train_tok)
X_test_tok = pd.DataFrame(X_test_tok)
df_train = data.drop('Text', axis = 1).merge(X_train_tok, left_index=True, right_index=True)
df_test = test.drop('Text', axis = 1).merge(X_test_tok, left_index=True, right_index=True)

# Label Encode Subject
le = LabelEncoder()
df_train['Subject'] = le.fit_transform(df_train['Subject'])
df_test['Subject'] = le.fit_transform(df_test['Subject'])

df_train = df_train.fillna(0)
df_test = df_test.fillna(0)

# split the dataset into training and testing sets
y_train = df_train['Sentiment']
X_train = df_train.drop('Sentiment', axis = 1)
y_test = df_test['Sentiment']
X_test = df_test.drop('Sentiment', axis = 1)

X_train_tok = X_train.to_numpy()
X_test_tok = X_test.to_numpy()

max_seq_len = 500

# clip sequences
X_train_tok = [s[:max_seq_len] for s in X_train_tok]
X_test_tok = [s[:max_seq_len] for s in X_test_tok]

# create masks
mask_train = [[1]*len(s)+[0]*(max_seq_len-len(s)) for s in X_train_tok]
mask_test = [[1]*len(s)+[0]*(max_seq_len-len(s)) for s in X_test_tok]

# pad sequences
# X_train_tok = [s+[0]*(max_seq_len-len(s)) for s in X_train_tok]
# X_test_tok = [s+[0]*(max_seq_len-len(s)) for s in X_test_tok]

train_dset = torch.utils.data.TensorDataset(torch.tensor(X_train_tok, 
                                                         dtype=torch.long), 
                                            torch.tensor(y_train, 
                                                         dtype=torch.long), 
                                            torch.tensor(mask_train, 
                                                         dtype=torch.float))

test_dset = torch.utils.data.TensorDataset(torch.tensor(X_test_tok, 
                                                        dtype=torch.long), 
                                          torch.tensor(y_test, 
                                                        dtype=torch.long), 
                                          torch.tensor(mask_test, 
                                                        dtype=torch.float))

batch_size = 128

train_loader = torch.utils.data.DataLoader(train_dset,
                          batch_size=batch_size,
                          shuffle=True,
                          num_workers=2
                         )

test_loader = torch.utils.data.DataLoader(test_dset,
                          batch_size=batch_size,
                          shuffle=False,
                          num_workers=2
                         )

# demo
x, y, m = next(iter(train_loader))
print(x.shape, y.shape, m.shape)
# x.shape: [B, L], y.shape: [L], mask.shape: [B, L]
print(x)
print(y)
print(m)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class RNNClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, vocab_size, \
                 num_rec_layers=1, rec_layer=nn.LSTM):
        super(RNNClassifier, self).__init__()
        # define all layers we need, 
        # their parameters will be initialized automatically
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.rnn1 = rec_layer(embedding_dim, hidden_dim, batch_first=True)
        self.num_rec_layers = num_rec_layers
        if self.num_rec_layers == 2:
            self.rnn2 = rec_layer(hidden_dim, hidden_dim, batch_first=True)
            # this code with if is for simplicity, 
            # normally one would use torch.nn.ModuleList
        self.hidden2label = nn.Linear(hidden_dim, 1)
    
    def forward(self, sentences, mask):
        # sentences shape: [B, L], mask shape: [B, L]
        embedding = self.word_embeddings(sentences) # shape: [B, L, DE]
        out, hidden = self.rnn1(embedding) # out shape: [B, L, DH]
        if self.num_rec_layers == 2:
            out, hidden = self.rnn2(out) # out shape: [B, L, DH]
        out = (out*mask[:, :, None]).mean(dim=1) # shape: [B, DH]
        res = self.hidden2label(out) # shape: [B, 1]
        return torch.sigmoid(res)

# create a particular instance of the model 
rnn = RNNClassifier(128, 128, tokenizer.vocab_size)
rnn.to(device)

