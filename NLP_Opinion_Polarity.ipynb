{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fjAIj8EPh4u3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.23.5\n",
        "\n",
        "#Restart Runtime after this is done\n",
        "# Just run it once, then disconnect and delete runtime, then skip this cell, you don't need it anymore\n",
        "\n",
        "!pip uninstall -y pandas\n",
        "!pip install pandas==1.5.3 \n",
        "\n",
        "!pip uninstall -y scikit-learn\n",
        "!pip install scikit-learn==1.2.0\n",
        "\n",
        "!pip uninstall -y nltk\n",
        "!pip install nltk==3.8.1\n",
        "\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.13.1\n",
        "\n",
        "# Just run it once, then disconnect and delete runtime, then skip this cell, you don't need it anymore\n",
        "\n",
        "!pip install pytorch-lightning==1.8.1\n",
        "!pip install transformers==4.22.2\n",
        "!pip install datasets==2.9.0 \n",
        "!pip install sentencepiece==0.1.97\n",
        "!pip install stanza==1.4.2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7czqanXZWRrf",
        "outputId": "f3b11b82-e8cc-4057-b1c3-cca69965a071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.22.4\n",
            "Uninstalling numpy-1.22.4:\n",
            "  Successfully uninstalled numpy-1.22.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "Successfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 1.5.3\n",
            "Uninstalling pandas-1.5.3:\n",
            "  Successfully uninstalled pandas-1.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "Successfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 1.2.2\n",
            "Uninstalling scikit-learn-1.2.2:\n",
            "  Successfully uninstalled scikit-learn-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn==1.2.0\n",
            "  Downloading scikit_learn-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (1.23.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (3.1.0)\n",
            "Installing collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nltk 3.8.1\n",
            "Uninstalling nltk-3.8.1:\n",
            "  Successfully uninstalled nltk-3.8.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (8.1.3)\n",
            "Installing collected packages: nltk\n",
            "Successfully installed nltk-3.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.0+cu118\n",
            "Uninstalling torch-2.0.0+cu118:\n",
            "  Successfully uninstalled torch-2.0.0+cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (4.5.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.6.1)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning==1.8.1\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.4/798.4 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (4.5.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (1.13.1)\n",
            "Collecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (4.65.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (2.12.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (1.23.5)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (23.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (6.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.40.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (2.17.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.4.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (67.6.1)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.53.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.8.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (11.7.99)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (2.0.12)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (6.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (2.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning==1.8.1) (2.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=567b94adfe4e3c9eafa018cfbfe5aa7f026d257c88cf03afd1fcb6a2ca586bf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built fire\n",
            "Installing collected packages: multidict, frozenlist, fire, async-timeout, yarl, lightning-utilities, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 fire-0.5.0 frozenlist-1.3.3 lightning-utilities-0.3.0 multidict-6.0.4 pytorch-lightning-1.8.1 torchmetrics-0.11.4 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.22.2\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.2) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.12.1 transformers-4.22.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.9.0\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (1.5.3)\n",
            "Collecting dill<0.3.7\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (6.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (2023.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (0.13.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (1.23.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (2.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.9.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.9.0) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.9.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.9.0) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.9.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanza==1.4.2\n",
            "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.3/691.3 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (4.65.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (2.27.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (3.20.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (11.10.3.66)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza==1.4.2) (67.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza==1.4.2) (0.40.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (2022.12.7)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=b0f531d1e118005b57eae4bc72cfbb9f82b5bf90ebc2974e9f99285b70566e75\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.2.0 stanza-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AvRbbRrFjip",
        "outputId": "c0cf7d98-2a97-447b-f973-3cd5834ff132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------------------\n",
            "absl-py                       1.4.0\n",
            "aiohttp                       3.8.4\n",
            "aiosignal                     1.3.1\n",
            "alabaster                     0.7.13\n",
            "albumentations                1.2.1\n",
            "altair                        4.2.2\n",
            "anyio                         3.6.2\n",
            "appdirs                       1.4.4\n",
            "argon2-cffi                   21.3.0\n",
            "argon2-cffi-bindings          21.2.0\n",
            "arviz                         0.15.1\n",
            "astropy                       5.2.2\n",
            "astunparse                    1.6.3\n",
            "async-timeout                 4.0.2\n",
            "attrs                         22.2.0\n",
            "audioread                     3.0.0\n",
            "autograd                      1.5\n",
            "Babel                         2.12.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.11.2\n",
            "bleach                        6.0.0\n",
            "blis                          0.7.9\n",
            "blosc2                        2.0.0\n",
            "bokeh                         2.4.3\n",
            "branca                        0.6.0\n",
            "CacheControl                  0.12.11\n",
            "cached-property               1.5.2\n",
            "cachetools                    5.3.0\n",
            "catalogue                     2.0.8\n",
            "certifi                       2022.12.7\n",
            "cffi                          1.15.1\n",
            "chardet                       4.0.0\n",
            "charset-normalizer            2.0.12\n",
            "chex                          0.1.7\n",
            "click                         8.1.3\n",
            "cloudpickle                   2.2.1\n",
            "cmake                         3.25.2\n",
            "cmdstanpy                     1.1.0\n",
            "colorcet                      3.0.1\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "confection                    0.0.4\n",
            "cons                          0.4.5\n",
            "contextlib2                   0.6.0.post1\n",
            "contourpy                     1.0.7\n",
            "convertdate                   2.4.0\n",
            "cryptography                  40.0.1\n",
            "cufflinks                     0.17.3\n",
            "cupy-cuda11x                  11.0.0\n",
            "cvxopt                        1.3.0\n",
            "cvxpy                         1.3.1\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.34\n",
            "dask                          2022.12.1\n",
            "datascience                   0.17.6\n",
            "datasets                      2.9.0\n",
            "db-dtypes                     1.1.1\n",
            "dbus-python                   1.2.16\n",
            "debugpy                       1.6.6\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "dill                          0.3.6\n",
            "distributed                   2022.12.1\n",
            "dlib                          19.24.1\n",
            "dm-tree                       0.1.8\n",
            "docutils                      0.16\n",
            "dopamine-rl                   4.0.6\n",
            "earthengine-api               0.1.348\n",
            "easydict                      1.10\n",
            "ecos                          2.0.12\n",
            "editdistance                  0.6.2\n",
            "emoji                         2.2.0\n",
            "en-core-web-sm                3.5.0\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.4\n",
            "et-xmlfile                    1.1.0\n",
            "etils                         1.2.0\n",
            "etuples                       0.3.8\n",
            "exceptiongroup                1.1.1\n",
            "fastai                        2.7.12\n",
            "fastcore                      1.5.29\n",
            "fastdownload                  0.0.7\n",
            "fastjsonschema                2.16.3\n",
            "fastprogress                  1.0.3\n",
            "fastrlock                     0.8.1\n",
            "filelock                      3.11.0\n",
            "fire                          0.5.0\n",
            "firebase-admin                5.3.0\n",
            "Flask                         2.2.3\n",
            "flatbuffers                   23.3.3\n",
            "flax                          0.6.8\n",
            "folium                        0.14.0\n",
            "fonttools                     4.39.3\n",
            "frozendict                    2.3.7\n",
            "frozenlist                    1.3.3\n",
            "fsspec                        2023.4.0\n",
            "future                        0.18.3\n",
            "gast                          0.4.0\n",
            "GDAL                          3.3.2\n",
            "gdown                         4.6.6\n",
            "gensim                        4.3.1\n",
            "geographiclib                 2.0\n",
            "geopy                         2.3.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               2.11.0\n",
            "google-api-python-client      2.84.0\n",
            "google-auth                   2.17.2\n",
            "google-auth-httplib2          0.1.0\n",
            "google-auth-oauthlib          1.0.0\n",
            "google-cloud-bigquery         3.9.0\n",
            "google-cloud-bigquery-storage 2.19.1\n",
            "google-cloud-core             2.3.2\n",
            "google-cloud-datastore        2.15.1\n",
            "google-cloud-firestore        2.11.0\n",
            "google-cloud-language         2.9.1\n",
            "google-cloud-storage          2.8.0\n",
            "google-cloud-translate        3.11.1\n",
            "google-colab                  1.0.0\n",
            "google-crc32c                 1.5.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        2.4.1\n",
            "googleapis-common-protos      1.59.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.20.1\n",
            "greenlet                      2.0.2\n",
            "grpcio                        1.53.0\n",
            "grpcio-status                 1.48.2\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.25.2\n",
            "gym-notices                   0.0.8\n",
            "h5netcdf                      1.1.0\n",
            "h5py                          3.8.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.4\n",
            "holidays                      0.22\n",
            "holoviews                     1.15.4\n",
            "html5lib                      1.1\n",
            "httpimport                    1.3.0\n",
            "httplib2                      0.21.0\n",
            "huggingface-hub               0.13.4\n",
            "humanize                      4.6.0\n",
            "hyperopt                      0.2.7\n",
            "idna                          3.4\n",
            "imageio                       2.25.1\n",
            "imageio-ffmpeg                0.4.8\n",
            "imagesize                     1.4.1\n",
            "imbalanced-learn              0.10.1\n",
            "imgaug                        0.4.0\n",
            "importlib-metadata            6.3.0\n",
            "importlib-resources           5.12.0\n",
            "imutils                       0.5.4\n",
            "inflect                       6.0.4\n",
            "iniconfig                     2.0.0\n",
            "intel-openmp                  2023.1.0\n",
            "ipykernel                     5.5.6\n",
            "ipython                       7.34.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.4.1\n",
            "ipywidgets                    7.7.1\n",
            "itsdangerous                  2.1.2\n",
            "jax                           0.4.8\n",
            "jaxlib                        0.4.7+cuda11.cudnn86\n",
            "jieba                         0.42.1\n",
            "Jinja2                        3.1.2\n",
            "joblib                        1.2.0\n",
            "jsonpickle                    3.0.1\n",
            "jsonschema                    4.3.3\n",
            "jupyter-client                6.1.12\n",
            "jupyter-console               6.1.0\n",
            "jupyter_core                  5.3.0\n",
            "jupyter-server                1.23.6\n",
            "jupyterlab-pygments           0.2.2\n",
            "jupyterlab-widgets            3.0.7\n",
            "kaggle                        1.5.13\n",
            "keras                         2.12.0\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.4.4\n",
            "korean-lunar-calendar         0.3.1\n",
            "langcodes                     3.3.0\n",
            "lazy_loader                   0.2\n",
            "libclang                      16.0.0\n",
            "librosa                       0.10.0.post2\n",
            "lightgbm                      3.3.5\n",
            "lightning-utilities           0.3.0\n",
            "lit                           16.0.1\n",
            "llvmlite                      0.39.1\n",
            "locket                        1.0.0\n",
            "logical-unification           0.4.5\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.9.2\n",
            "Markdown                      3.4.3\n",
            "markdown-it-py                2.2.0\n",
            "MarkupSafe                    2.1.2\n",
            "matplotlib                    3.7.1\n",
            "matplotlib-inline             0.1.6\n",
            "matplotlib-venn               0.11.9\n",
            "mdurl                         0.1.2\n",
            "miniKanren                    1.0.3\n",
            "missingno                     0.5.2\n",
            "mistune                       0.8.4\n",
            "mizani                        0.8.1\n",
            "mkl                           2019.0\n",
            "ml-dtypes                     0.0.4\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                9.1.0\n",
            "moviepy                       1.0.3\n",
            "mpmath                        1.3.0\n",
            "msgpack                       1.0.5\n",
            "multidict                     6.0.4\n",
            "multipledispatch              0.6.0\n",
            "multiprocess                  0.70.14\n",
            "multitasking                  0.0.11\n",
            "murmurhash                    1.0.9\n",
            "music21                       8.1.0\n",
            "natsort                       8.3.1\n",
            "nbclient                      0.7.3\n",
            "nbconvert                     6.5.4\n",
            "nbformat                      5.8.0\n",
            "nest-asyncio                  1.5.6\n",
            "networkx                      3.1\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.8.1\n",
            "notebook                      6.4.8\n",
            "numba                         0.56.4\n",
            "numexpr                       2.8.4\n",
            "numpy                         1.23.5\n",
            "nvidia-cublas-cu11            11.10.3.66\n",
            "nvidia-cuda-nvrtc-cu11        11.7.99\n",
            "nvidia-cuda-runtime-cu11      11.7.99\n",
            "nvidia-cudnn-cu11             8.5.0.96\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.2\n",
            "opencv-contrib-python         4.7.0.72\n",
            "opencv-python                 4.7.0.72\n",
            "opencv-python-headless        4.7.0.72\n",
            "openpyxl                      3.0.10\n",
            "opt-einsum                    3.3.0\n",
            "optax                         0.1.4\n",
            "orbax                         0.1.7\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     23.0\n",
            "palettable                    3.3.1\n",
            "pandas                        1.5.3\n",
            "pandas-datareader             0.10.0\n",
            "pandas-gbq                    0.17.9\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.14.4\n",
            "param                         1.13.0\n",
            "parso                         0.8.3\n",
            "partd                         1.3.0\n",
            "pathlib                       1.0.1\n",
            "pathy                         0.10.1\n",
            "patsy                         0.5.3\n",
            "pep517                        0.13.0\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        8.4.0\n",
            "pip                           23.0.1\n",
            "pip-tools                     6.6.2\n",
            "platformdirs                  3.2.0\n",
            "plotly                        5.13.1\n",
            "plotnine                      0.10.1\n",
            "pluggy                        1.0.0\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.3\n",
            "preshed                       3.0.8\n",
            "prettytable                   0.7.2\n",
            "proglog                       0.1.10\n",
            "progressbar2                  4.2.0\n",
            "prometheus-client             0.16.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                3.0.38\n",
            "prophet                       1.1.2\n",
            "proto-plus                    1.22.2\n",
            "protobuf                      3.20.3\n",
            "psutil                        5.9.4\n",
            "psycopg2                      2.9.6\n",
            "ptyprocess                    0.7.0\n",
            "py-cpuinfo                    9.0.0\n",
            "py4j                          0.10.9.7\n",
            "pyarrow                       9.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.6\n",
            "pycparser                     2.21\n",
            "pyct                          0.5.0\n",
            "pydantic                      1.10.7\n",
            "pydata-google-auth            1.7.0\n",
            "pydot                         1.4.2\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyerfa                        2.0.0.3\n",
            "pygame                        2.3.0\n",
            "Pygments                      2.14.0\n",
            "PyGObject                     3.36.0\n",
            "pymc                          5.1.2\n",
            "PyMeeus                       0.5.12\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.9\n",
            "pyrsistent                    0.19.3\n",
            "PySocks                       1.7.1\n",
            "pytensor                      2.10.1\n",
            "pytest                        7.2.2\n",
            "python-apt                    0.0.0\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.16\n",
            "python-slugify                8.0.1\n",
            "python-utils                  3.5.2\n",
            "pytorch-lightning             1.8.1\n",
            "pytz                          2022.7.1\n",
            "pytz-deprecation-shim         0.1.0.post0\n",
            "pyviz-comms                   2.2.1\n",
            "PyWavelets                    1.4.1\n",
            "PyYAML                        6.0\n",
            "pyzmq                         23.2.1\n",
            "qdldl                         0.1.7\n",
            "qudida                        0.0.4\n",
            "regex                         2022.10.31\n",
            "requests                      2.27.1\n",
            "requests-oauthlib             1.3.1\n",
            "requests-unixsocket           0.2.0\n",
            "responses                     0.18.0\n",
            "rich                          13.3.3\n",
            "rpy2                          3.5.5\n",
            "rsa                           4.9\n",
            "scikit-image                  0.19.3\n",
            "scikit-learn                  1.2.0\n",
            "scipy                         1.10.1\n",
            "scs                           3.2.3\n",
            "seaborn                       0.12.2\n",
            "Send2Trash                    1.8.0\n",
            "sentencepiece                 0.1.97\n",
            "setuptools                    67.6.1\n",
            "shapely                       2.0.1\n",
            "six                           1.16.0\n",
            "sklearn-pandas                2.2.0\n",
            "smart-open                    6.3.0\n",
            "sniffio                       1.3.0\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "soundfile                     0.12.1\n",
            "soupsieve                     2.4\n",
            "soxr                          0.3.5\n",
            "spacy                         3.5.1\n",
            "spacy-legacy                  3.0.12\n",
            "spacy-loggers                 1.0.4\n",
            "Sphinx                        3.5.4\n",
            "sphinxcontrib-applehelp       1.0.4\n",
            "sphinxcontrib-devhelp         1.0.2\n",
            "sphinxcontrib-htmlhelp        2.0.1\n",
            "sphinxcontrib-jsmath          1.0.1\n",
            "sphinxcontrib-qthelp          1.0.3\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "SQLAlchemy                    2.0.9\n",
            "sqlparse                      0.4.3\n",
            "srsly                         2.4.6\n",
            "stanza                        1.4.2\n",
            "statsmodels                   0.13.5\n",
            "sympy                         1.11.1\n",
            "tables                        3.8.0\n",
            "tabulate                      0.8.10\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.2.2\n",
            "tensorboard                   2.12.1\n",
            "tensorboard-data-server       0.7.0\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.12.0\n",
            "tensorflow-datasets           4.8.3\n",
            "tensorflow-estimator          2.12.0\n",
            "tensorflow-gcs-config         2.12.0\n",
            "tensorflow-hub                0.13.0\n",
            "tensorflow-io-gcs-filesystem  0.32.0\n",
            "tensorflow-metadata           1.13.0\n",
            "tensorflow-probability        0.19.0\n",
            "tensorstore                   0.1.35\n",
            "termcolor                     2.2.0\n",
            "terminado                     0.17.1\n",
            "text-unidecode                1.3\n",
            "textblob                      0.17.1\n",
            "tf-slim                       1.1.0\n",
            "thinc                         8.1.9\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2023.3.21\n",
            "tinycss2                      1.2.1\n",
            "tokenizers                    0.12.1\n",
            "toml                          0.10.2\n",
            "tomli                         2.0.1\n",
            "toolz                         0.12.0\n",
            "torch                         1.13.1\n",
            "torchaudio                    2.0.1+cu118\n",
            "torchdata                     0.6.0\n",
            "torchmetrics                  0.11.4\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.15.1\n",
            "torchvision                   0.15.1+cu118\n",
            "tornado                       6.2\n",
            "tqdm                          4.65.0\n",
            "traitlets                     5.7.1\n",
            "transformers                  4.22.2\n",
            "triton                        2.0.0\n",
            "tweepy                        4.13.0\n",
            "typer                         0.7.0\n",
            "typing_extensions             4.5.0\n",
            "tzdata                        2023.3\n",
            "tzlocal                       4.3\n",
            "uritemplate                   4.1.1\n",
            "urllib3                       1.26.15\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        1.1.1\n",
            "wcwidth                       0.2.6\n",
            "webcolors                     1.13\n",
            "webencodings                  0.5.1\n",
            "websocket-client              1.5.1\n",
            "Werkzeug                      2.2.3\n",
            "wheel                         0.40.0\n",
            "widgetsnbextension            3.6.4\n",
            "wordcloud                     1.8.2.2\n",
            "wrapt                         1.14.1\n",
            "xarray                        2022.12.0\n",
            "xarray-einstats               0.5.1\n",
            "xgboost                       1.7.5\n",
            "xlrd                          2.0.1\n",
            "xxhash                        3.2.0\n",
            "yarl                          1.8.2\n",
            "yellowbrick                   1.5\n",
            "yfinance                      0.2.17\n",
            "zict                          2.2.0\n",
            "zipp                          3.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string\n",
        "import nltk.data\n",
        "import nltk\n",
        "from matplotlib import pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# Download English \n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUod1NFbuhtl",
        "outputId": "d907b18b-224c-48d2-ce5c-be4c66511e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_string(text):\n",
        "  # 1. remove tags\n",
        "  text = re.sub(re.compile('<.*?>'), ' ', text)\n",
        "  # 2. remove punctuation and lowercase\n",
        "  text = re.sub(r'[^\\w\\s\\']',' ', text).lower()\n",
        "  # 3. remove duplicate spaces\n",
        "  text = re.sub(' +', ' ', text)\n",
        "  return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def preprocess(data):\n",
        "    #renaming the columns \n",
        "    data = data.rename(columns={0:'Sentiment',1:'Category',2:'Subject',3:'Index',4:'Text'})\n",
        "\n",
        "    # Mark the subject\n",
        "    data['Text'] = data.apply(lambda x: x['Text'][:int(x.Index.split(':')[0])]+'thisiswherethesubjectissupposedtobe'+x['Text'][int(x.Index.split(':')[1]):],axis=1)\n",
        "\n",
        "    # preprocess\n",
        "    data[\"Text\"] = data[\"Text\"].apply(process_string)\n",
        "\n",
        "    # Separating 2 columns\n",
        "    data[['Main_Category','Sub_Category']] = data['Category'].str.split('#', 1, expand=True)\n",
        "    data = data.drop(columns='Category')\n",
        "\n",
        "    # Label Encoding Sentiment, Category, and Sub-Category\n",
        "    data['Sentiment'] = data['Sentiment'].apply(lambda x: 1 if x == 'positive' else (0 if x == 'neutral' else -1))\n",
        "    data['Main_Category_Label'] = data['Main_Category'].apply(lambda x: 1 if x == 'AMBIENCE' else (2 if x == 'FOOD' else (3 if x == 'SERVICE' else (4 if x == 'RESTAURANT' else (5 if x == 'DRINKS' else 6)))))\n",
        "    data['Sub_Category_Label'] = data['Sub_Category'].apply(lambda x: 1 if x == 'GENERAL' else (2 if x == 'QUALITY' else (3 if x == 'STYLE_OPTIONS' else (4 if x == 'MISCELLANEOUS' else 5))))\n",
        "\n",
        "    # pre-process Subject column\n",
        "    data[\"Subject\"] = data[\"Subject\"].apply(process_string)\n",
        "\n",
        "    # Remove stopwords from Text and Subject\n",
        "    #data['Text'] = data['Text'].apply(remove_stopwords)\n",
        "    #data['Subject'] = data['Subject'].apply(remove_stopwords)\n",
        "\n",
        "    # Changing Index column\n",
        "    data['Index'] = data.apply(lambda x: str(x['Text'].find('thisiswherethesubjectissupposedtobe'))+':'+str(x['Text'].find('thisiswherethesubjectissupposedtobe')+ len(x['Subject'])), axis = 1)\n",
        "    data['Text'] = data.apply(lambda x: x['Text'].replace('thisiswherethesubjectissupposedtobe',x['Subject']), axis = 1)\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Get stopwords \n",
        "stopwords_ENG = stopwords.words('english')\n",
        "stop_words = set(stopwords_ENG)"
      ],
      "metadata": {
        "id": "GPfTYCcGYJLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "train = pd.read_csv('traindata.csv',sep='\t',header=None)\n",
        "\n",
        "#train = preprocess(train)\n",
        "\n",
        "test = pd.read_csv('devdata.csv',sep='\t',header=None)\n",
        "\n",
        "#test = preprocess(test)"
      ],
      "metadata": {
        "id": "1vIoaSjjYK6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.iloc[[7,   53,   60,   91,  183,  195,  197,  369,  431,  469,  475,  614,  647,  666,  670,  870,  876,  916,  928,  950,  988, 1153,  1233, 1246, 1276, 1388, 1496]]"
      ],
      "metadata": {
        "id": "OnG8IBDzmWLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a81299f9-2847-4be0-c44a-2a84bad14d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0                         1                 2        3  \\\n",
              "7     negative           SERVICE#GENERAL           service    76:83   \n",
              "53    positive              FOOD#QUALITY              food      4:8   \n",
              "60    positive              FOOD#QUALITY           lobster    92:99   \n",
              "91    negative           SERVICE#GENERAL          waitress    19:27   \n",
              "183   positive              FOOD#QUALITY              good    63:67   \n",
              "195   negative         RESTAURANT#PRICES  The Four Seasons     0:16   \n",
              "197   negative           SERVICE#GENERAL          waitress     4:12   \n",
              "369   positive  RESTAURANT#MISCELLANEOUS             place    85:90   \n",
              "431   negative              FOOD#QUALITY              dish  112:116   \n",
              "469   positive              FOOD#QUALITY             pizza      4:9   \n",
              "475    neutral  RESTAURANT#MISCELLANEOUS  The Four Seasons     0:16   \n",
              "614   positive           SERVICE#GENERAL           service     4:11   \n",
              "647   negative          AMBIENCE#GENERAL    mirrored walls    57:71   \n",
              "666   positive        RESTAURANT#GENERAL             place    10:15   \n",
              "670   positive              FOOD#QUALITY             filet    61:66   \n",
              "870   negative               FOOD#PRICES              food      4:8   \n",
              "876   positive              FOOD#QUALITY             sushi    29:34   \n",
              "916   positive              FOOD#QUALITY             rolls    16:21   \n",
              "928   positive              FOOD#QUALITY              Food      0:4   \n",
              "950   positive              FOOD#QUALITY            caviar    72:78   \n",
              "988   positive              FOOD#QUALITY       ingredients    20:31   \n",
              "1153  positive              FOOD#QUALITY             pizza    17:22   \n",
              "1233  negative              FOOD#QUALITY            salmon    29:35   \n",
              "1246  positive              FOOD#QUALITY              food      5:9   \n",
              "1276  positive              FOOD#QUALITY             sushi  147:152   \n",
              "1388  positive              FOOD#QUALITY             bagel    63:68   \n",
              "1496  negative           SERVICE#GENERAL           service    33:40   \n",
              "\n",
              "                                                                                                                                                                                                                     4  \n",
              "7                                                                                                                                 sometimes i get bad food and bad service, sometimes i get good good and bad service.  \n",
              "53                                                                                                The food is a diamond in rough -- the food is delicious and homemade with the perfect balance of herbs and tomatoes.  \n",
              "60                                                                                                                I have to highly recommend the lobster roll - not to much mayo; you can tell it was a fresh lobster.  \n",
              "91                      At this point, the waitress comes over and asks us if everything was okay, I was literally so shocked that I was speechless and didn't say anything, and guess what, the waitress WALKED away.  \n",
              "183                                                                                                                               sometimes i get bad food and bad service, sometimes i get good good and bad service.  \n",
              "195   The Four Seasons has history and it is a sort of landmark of New York City restaurants, but trust me, they will charge you through the nose just so that you can say \"I've been to the four seasons restaurant\".  \n",
              "197                                                                                                                                The waitress, seems to be more concerned of looking good than actually waitressing.  \n",
              "369                                                            It was the first place we ate on our first trip to New York, and it will be the last place we stop as we head out of town on our next trip to New York.  \n",
              "431                                                     I've enjoyed 99% of the dishes we've ordered with the only exceptions being the occasional too-authentic-for-me dish (I'm a daring eater but not THAT daring).  \n",
              "469                                                                                        The pizza is delicious - they use fresh mozzarella instead of the cheap, frozen, shredded cheese common to most pizzaria's.  \n",
              "475   The Four Seasons has history and it is a sort of landmark of New York City restaurants, but trust me, they will charge you through the nose just so that you can say \"I've been to the four seasons restaurant\".  \n",
              "614                                                            The service was extremely fast and attentive(thanks to the service button on your table) but I barely understood 1 word when the waiter took our order.  \n",
              "647                  This place has totally weird decor, stairs going up with mirrored walls - I am surprised how no one yet broke their head or fall off the stairs - mirrored walls make you dizzy and delusional...  \n",
              "666                                                                                                                             Love this place, every time we are in the city this is one of the places we always go.  \n",
              "670                                         Guacamole+shrimp appetizer was really great, we both had the filet, very good, didn't much like the frites that came with, but the filet was so good, neither of us cared.  \n",
              "870                                                The food can get pricey but the prixe fixe tasting menu is the greatest food for a good price and they cater the food to any food allergies or food you don't like.  \n",
              "876     We are very particular about sushi and were both please with every choice which included: ceviche mix (special), crab dumplings, assorted sashimi, sushi and rolls, two types of sake, and the banana tempura.  \n",
              "916                                                                                                                             They have great rolls, the triple color and norwegetan rolls, are awesome and filling.  \n",
              "928                                                                                                  Food was amazing - I love Indian food and eat it quite regularly, but I can say this is one of the best I've had.  \n",
              "950                                                                                                              I highly recommend Caviar Russe to anyone who wants delicious top grade caviar and fantastic service.  \n",
              "988                                                                             Yes, they use fancy ingredients, but even fancy ingredients don't make for good pizza unless someone knows how to get the crust right.  \n",
              "1153                                                                                     Pizza - the only pizza in NYC that should not have additional toppings - the crust tastes like the best, freshly baked bread!  \n",
              "1233                                I took one bite from the $24 salmon, and I have never, in the 17 years I have been going to restaurants tasted salmon as fishy, as dry, and as bland as the one in Flatbush Farms.  \n",
              "1246                                                                                                                                                                  Good food: my favorite is the seafood spaghetti.  \n",
              "1276    We are very particular about sushi and were both please with every choice which included: ceviche mix (special), crab dumplings, assorted sashimi, sushi and rolls, two types of sake, and the banana tempura.  \n",
              "1388                                                                                                                                       Ess-A-Bagel (either by Sty-town or midtown) is by far the best bagel in NY.  \n",
              "1496                                                                                                                              sometimes i get bad food and bad service, sometimes i get good good and bad service.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c20ef502-5938-47ab-934e-8cb79397c712\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>service</td>\n",
              "      <td>76:83</td>\n",
              "      <td>sometimes i get bad food and bad service, sometimes i get good good and bad service.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>food</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The food is a diamond in rough -- the food is delicious and homemade with the perfect balance of herbs and tomatoes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>lobster</td>\n",
              "      <td>92:99</td>\n",
              "      <td>I have to highly recommend the lobster roll - not to much mayo; you can tell it was a fresh lobster.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>waitress</td>\n",
              "      <td>19:27</td>\n",
              "      <td>At this point, the waitress comes over and asks us if everything was okay, I was literally so shocked that I was speechless and didn't say anything, and guess what, the waitress WALKED away.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>good</td>\n",
              "      <td>63:67</td>\n",
              "      <td>sometimes i get bad food and bad service, sometimes i get good good and bad service.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>negative</td>\n",
              "      <td>RESTAURANT#PRICES</td>\n",
              "      <td>The Four Seasons</td>\n",
              "      <td>0:16</td>\n",
              "      <td>The Four Seasons has history and it is a sort of landmark of New York City restaurants, but trust me, they will charge you through the nose just so that you can say \"I've been to the four seasons restaurant\".</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>waitress</td>\n",
              "      <td>4:12</td>\n",
              "      <td>The waitress, seems to be more concerned of looking good than actually waitressing.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>positive</td>\n",
              "      <td>RESTAURANT#MISCELLANEOUS</td>\n",
              "      <td>place</td>\n",
              "      <td>85:90</td>\n",
              "      <td>It was the first place we ate on our first trip to New York, and it will be the last place we stop as we head out of town on our next trip to New York.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>negative</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>dish</td>\n",
              "      <td>112:116</td>\n",
              "      <td>I've enjoyed 99% of the dishes we've ordered with the only exceptions being the occasional too-authentic-for-me dish (I'm a daring eater but not THAT daring).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>pizza</td>\n",
              "      <td>4:9</td>\n",
              "      <td>The pizza is delicious - they use fresh mozzarella instead of the cheap, frozen, shredded cheese common to most pizzaria's.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>neutral</td>\n",
              "      <td>RESTAURANT#MISCELLANEOUS</td>\n",
              "      <td>The Four Seasons</td>\n",
              "      <td>0:16</td>\n",
              "      <td>The Four Seasons has history and it is a sort of landmark of New York City restaurants, but trust me, they will charge you through the nose just so that you can say \"I've been to the four seasons restaurant\".</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>positive</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>service</td>\n",
              "      <td>4:11</td>\n",
              "      <td>The service was extremely fast and attentive(thanks to the service button on your table) but I barely understood 1 word when the waiter took our order.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>negative</td>\n",
              "      <td>AMBIENCE#GENERAL</td>\n",
              "      <td>mirrored walls</td>\n",
              "      <td>57:71</td>\n",
              "      <td>This place has totally weird decor, stairs going up with mirrored walls - I am surprised how no one yet broke their head or fall off the stairs - mirrored walls make you dizzy and delusional...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>positive</td>\n",
              "      <td>RESTAURANT#GENERAL</td>\n",
              "      <td>place</td>\n",
              "      <td>10:15</td>\n",
              "      <td>Love this place, every time we are in the city this is one of the places we always go.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>filet</td>\n",
              "      <td>61:66</td>\n",
              "      <td>Guacamole+shrimp appetizer was really great, we both had the filet, very good, didn't much like the frites that came with, but the filet was so good, neither of us cared.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>870</th>\n",
              "      <td>negative</td>\n",
              "      <td>FOOD#PRICES</td>\n",
              "      <td>food</td>\n",
              "      <td>4:8</td>\n",
              "      <td>The food can get pricey but the prixe fixe tasting menu is the greatest food for a good price and they cater the food to any food allergies or food you don't like.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>sushi</td>\n",
              "      <td>29:34</td>\n",
              "      <td>We are very particular about sushi and were both please with every choice which included: ceviche mix (special), crab dumplings, assorted sashimi, sushi and rolls, two types of sake, and the banana tempura.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>916</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>rolls</td>\n",
              "      <td>16:21</td>\n",
              "      <td>They have great rolls, the triple color and norwegetan rolls, are awesome and filling.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>928</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>Food</td>\n",
              "      <td>0:4</td>\n",
              "      <td>Food was amazing - I love Indian food and eat it quite regularly, but I can say this is one of the best I've had.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>caviar</td>\n",
              "      <td>72:78</td>\n",
              "      <td>I highly recommend Caviar Russe to anyone who wants delicious top grade caviar and fantastic service.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>ingredients</td>\n",
              "      <td>20:31</td>\n",
              "      <td>Yes, they use fancy ingredients, but even fancy ingredients don't make for good pizza unless someone knows how to get the crust right.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1153</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>pizza</td>\n",
              "      <td>17:22</td>\n",
              "      <td>Pizza - the only pizza in NYC that should not have additional toppings - the crust tastes like the best, freshly baked bread!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>negative</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>salmon</td>\n",
              "      <td>29:35</td>\n",
              "      <td>I took one bite from the $24 salmon, and I have never, in the 17 years I have been going to restaurants tasted salmon as fishy, as dry, and as bland as the one in Flatbush Farms.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>food</td>\n",
              "      <td>5:9</td>\n",
              "      <td>Good food: my favorite is the seafood spaghetti.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1276</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>sushi</td>\n",
              "      <td>147:152</td>\n",
              "      <td>We are very particular about sushi and were both please with every choice which included: ceviche mix (special), crab dumplings, assorted sashimi, sushi and rolls, two types of sake, and the banana tempura.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>positive</td>\n",
              "      <td>FOOD#QUALITY</td>\n",
              "      <td>bagel</td>\n",
              "      <td>63:68</td>\n",
              "      <td>Ess-A-Bagel (either by Sty-town or midtown) is by far the best bagel in NY.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>negative</td>\n",
              "      <td>SERVICE#GENERAL</td>\n",
              "      <td>service</td>\n",
              "      <td>33:40</td>\n",
              "      <td>sometimes i get bad food and bad service, sometimes i get good good and bad service.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c20ef502-5938-47ab-934e-8cb79397c712')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c20ef502-5938-47ab-934e-8cb79397c712 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c20ef502-5938-47ab-934e-8cb79397c712');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old"
      ],
      "metadata": {
        "id": "fjAIj8EPh4u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Method 1: bag of words\n",
        "train = train.drop(columns=['Index','Main_Category','Sub_Category'])\n",
        "test = test.drop(columns=['Index','Main_Category','Sub_Category'])\n",
        "\n",
        "count_vect = CountVectorizer(stop_words='english', min_df=5)\n",
        "# Train\n",
        "X_train_counts = count_vect.fit_transform(train[\"Text\"])\n",
        "print(\"Train shape:\", X_train_counts.shape)\n",
        "\n",
        "# Test\n",
        "X_test_counts = count_vect.transform(test[\"Text\"])\n",
        "print(\"Test shape:\", X_test_counts.shape)\n",
        "\n",
        "# Train\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "X_train_vec = pd.DataFrame(X_train_counts.toarray(), columns = feature_names)\n",
        "df_train = train.drop('Text', axis = 1).merge(X_train_vec, left_index=True, right_index=True)\n",
        "\n",
        "# Test\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "X_test_vec = pd.DataFrame(X_test_counts.toarray(), columns = feature_names)\n",
        "df_test = test.drop('Text', axis = 1).merge(X_test_vec, left_index=True, right_index=True)\n",
        "\n",
        "# Label Encode Subject\n",
        "le = LabelEncoder()\n",
        "df_train['Subject'] = le.fit_transform(df_train['Subject'])\n",
        "df_test['Subject'] = le.fit_transform(df_test['Subject'])\n",
        "\n",
        "# -- we maybe need to work on classifier.py and tester.py\n",
        "# split the dataset into training and testing sets\n",
        "y_train = df_train['Sentiment']\n",
        "X_train = df_train.drop('Sentiment', axis = 1)\n",
        "y_test = df_test['Sentiment']\n",
        "X_test = df_test.drop('Sentiment', axis = 1)\n",
        "\n",
        "# train a logistic regression model\n",
        "model = LogisticRegression(max_iter = 10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the performance of the model on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, zero_division = True))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56K2t5r50LB3",
        "outputId": "34796807-96b6-4514-bc42-2a4cb7c0a4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1503, 508)\n",
            "Test shape: (376, 508)\n",
            "[[ 45   4  49]\n",
            " [  3   1  10]\n",
            " [ 17   1 246]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.69      0.46      0.55        98\n",
            "           0       0.17      0.07      0.10        14\n",
            "           1       0.81      0.93      0.86       264\n",
            "\n",
            "    accuracy                           0.78       376\n",
            "   macro avg       0.56      0.49      0.51       376\n",
            "weighted avg       0.75      0.78      0.75       376\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9KjDnrIV_l5",
        "outputId": "7c8e188c-bca2-4355-b306-7aaf729b3333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "[[ 40   2  56]\n",
            " [  4   0  10]\n",
            " [ 19   1 244]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.63      0.41      0.50        98\n",
            "           0       0.00      0.00      0.00        14\n",
            "           1       0.79      0.92      0.85       264\n",
            "\n",
            "    accuracy                           0.76       376\n",
            "   macro avg       0.47      0.44      0.45       376\n",
            "weighted avg       0.72      0.76      0.73       376\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Method 2: Word Embedding\n",
        "\n",
        "# a smaller model, faster download and no problems with out of memory\n",
        "import gensim.downloader as api\n",
        "wv = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "def text_to_vector(Text, wv):\n",
        "    vectors = []\n",
        "    for word in Text.split():\n",
        "        if word in wv: # skip our-of-vocabulary words\n",
        "            vectors.append(wv[word])\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "X_train_wv = np.zeros((train.shape[0], wv.vectors.shape[1]))\n",
        "for i, text in enumerate(train[\"Text\"]):\n",
        "    X_train_wv[i] = text_to_vector(text, wv)\n",
        "\n",
        "X_test_wv = np.zeros((test.shape[0], wv.vectors.shape[1]))\n",
        "for i, text in enumerate(test[\"Text\"]):\n",
        "    X_test_wv[i] = text_to_vector(text, wv)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_wv = scaler.fit_transform(X_train_wv)\n",
        "X_test_wv = scaler.transform(X_test_wv)\n",
        "\n",
        "X_train_emb = pd.DataFrame(X_train_wv)\n",
        "df_train = train.drop('Text', axis = 1).merge(X_train_emb, left_index=True, right_index=True)\n",
        "\n",
        "X_test_emb = pd.DataFrame(X_test_wv)\n",
        "df_test = test.drop('Text', axis = 1).merge(X_test_emb, left_index=True, right_index=True)\n",
        "\n",
        "# All column names as string\n",
        "df_train.columns = df_train.columns.astype(str)\n",
        "df_test.columns = df_test.columns.astype(str)\n",
        "\n",
        "# Label Encode Subject\n",
        "le = LabelEncoder()\n",
        "df_train['Subject'] = le.fit_transform(df_train['Subject'])\n",
        "df_test['Subject'] = le.fit_transform(df_test['Subject'])\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "y_train = df_train['Sentiment']\n",
        "X_train = df_train.drop('Sentiment', axis = 1)\n",
        "y_test = df_test['Sentiment']\n",
        "X_test = df_test.drop('Sentiment', axis = 1)\n",
        "\n",
        "# train a logistic regression model\n",
        "model = LogisticRegression(max_iter = 10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the performance of the model on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, zero_division = True))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Method 3: Recurrent Neural Network with Tokenization\n",
        "\n",
        "from transformers import GPT2Tokenizer \n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "X_train_tok = []\n",
        "for text in train[\"Text\"]:\n",
        "    X_train_tok.append(tokenizer.encode(text))\n",
        "\n",
        "X_test_tok = []\n",
        "for text in test[\"Text\"]:\n",
        "    X_test_tok.append(tokenizer.encode(text))\n",
        "\n",
        "print(X_train_tok[:3])\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "_ = plt.hist([len(s) for s in X_train_tok], 100)\n",
        "plt.xlabel(\"Length\")\n",
        "plt.ylabel(\"Number of sequences\")\n",
        "\n",
        "X_train_tok = pd.DataFrame(X_train_tok)\n",
        "X_test_tok = pd.DataFrame(X_test_tok)\n",
        "df_train = train.drop('Text', axis = 1).merge(X_train_tok, left_index=True, right_index=True)\n",
        "df_test = test.drop('Text', axis = 1).merge(X_test_tok, left_index=True, right_index=True)\n",
        "\n",
        "# Label Encode Subject\n",
        "le = LabelEncoder()\n",
        "df_train['Subject'] = le.fit_transform(df_train['Subject'])\n",
        "df_test['Subject'] = le.fit_transform(df_test['Subject'])\n",
        "\n",
        "df_train = df_train.fillna(0)\n",
        "df_test = df_test.fillna(0)\n",
        "\n",
        "# split the dataset into training and testing sets\n",
        "y_train = df_train['Sentiment']\n",
        "X_train = df_train.drop('Sentiment', axis = 1)\n",
        "y_test = df_test['Sentiment']\n",
        "X_test = df_test.drop('Sentiment', axis = 1)\n",
        "\n",
        "X_train_tok = X_train.to_numpy()\n",
        "X_test_tok = X_test.to_numpy()\n",
        "\n",
        "max_seq_len = 500\n",
        "\n",
        "# clip sequences\n",
        "X_train_tok = [s[:max_seq_len] for s in X_train_tok]\n",
        "X_test_tok = [s[:max_seq_len] for s in X_test_tok]\n",
        "\n",
        "# create masks\n",
        "mask_train = [[1]*len(s)+[0]*(max_seq_len-len(s)) for s in X_train_tok]\n",
        "mask_test = [[1]*len(s)+[0]*(max_seq_len-len(s)) for s in X_test_tok]\n",
        "\n",
        "# pad sequences\n",
        "# X_train_tok = [s+[0]*(max_seq_len-len(s)) for s in X_train_tok]\n",
        "# X_test_tok = [s+[0]*(max_seq_len-len(s)) for s in X_test_tok]\n",
        "\n",
        "train_dset = torch.utils.data.TensorDataset(torch.tensor(X_train_tok, \n",
        "                                                         dtype=torch.long), \n",
        "                                            torch.tensor(y_train, \n",
        "                                                         dtype=torch.long), \n",
        "                                            torch.tensor(mask_train, \n",
        "                                                         dtype=torch.float))\n",
        "\n",
        "test_dset = torch.utils.data.TensorDataset(torch.tensor(X_test_tok, \n",
        "                                                        dtype=torch.long), \n",
        "                                          torch.tensor(y_test, \n",
        "                                                        dtype=torch.long), \n",
        "                                          torch.tensor(mask_test, \n",
        "                                                        dtype=torch.float))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2\n",
        "                         )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2\n",
        "                         )\n",
        "\n",
        "# demo\n",
        "x, y, m = next(iter(train_loader))\n",
        "print(x.shape, y.shape, m.shape)\n",
        "# x.shape: [B, L], y.shape: [L], mask.shape: [B, L]\n",
        "print(x)\n",
        "print(y)\n",
        "print(m)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, \\\n",
        "                 num_rec_layers=1, rec_layer=nn.LSTM):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        # define all layers we need, \n",
        "        # their parameters will be initialized automatically\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn1 = rec_layer(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.num_rec_layers = num_rec_layers\n",
        "        if self.num_rec_layers == 2:\n",
        "            self.rnn2 = rec_layer(hidden_dim, hidden_dim, batch_first=True)\n",
        "            # this code with if is for simplicity, \n",
        "            # normally one would use torch.nn.ModuleList\n",
        "        self.hidden2label = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, sentences, mask):\n",
        "        # sentences shape: [B, L], mask shape: [B, L]\n",
        "        embedding = self.word_embeddings(sentences) # shape: [B, L, DE]\n",
        "        out, hidden = self.rnn1(embedding) # out shape: [B, L, DH]\n",
        "        if self.num_rec_layers == 2:\n",
        "            out, hidden = self.rnn2(out) # out shape: [B, L, DH]\n",
        "        out = (out*mask[:, :, None]).mean(dim=1) # shape: [B, DH]\n",
        "        res = self.hidden2label(out) # shape: [B, 1]\n",
        "        return torch.sigmoid(res)\n",
        "\n",
        "# create a particular instance of the model \n",
        "rnn = RNNClassifier(128, 128, tokenizer.vocab_size)\n",
        "rnn.to(device)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "-2wQ61P50UOK",
        "outputId": "c927a175-f310-4dd7-f221-7eb9ed66771d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Method 3: Recurrent Neural Network with Tokenization\\n\\nfrom transformers import GPT2Tokenizer \\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\\n\\nX_train_tok = []\\nfor text in train[\"Text\"]:\\n    X_train_tok.append(tokenizer.encode(text))\\n\\nX_test_tok = []\\nfor text in test[\"Text\"]:\\n    X_test_tok.append(tokenizer.encode(text))\\n\\nprint(X_train_tok[:3])\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\n\\n_ = plt.hist([len(s) for s in X_train_tok], 100)\\nplt.xlabel(\"Length\")\\nplt.ylabel(\"Number of sequences\")\\n\\nX_train_tok = pd.DataFrame(X_train_tok)\\nX_test_tok = pd.DataFrame(X_test_tok)\\ndf_train = train.drop(\\'Text\\', axis = 1).merge(X_train_tok, left_index=True, right_index=True)\\ndf_test = test.drop(\\'Text\\', axis = 1).merge(X_test_tok, left_index=True, right_index=True)\\n\\n# Label Encode Subject\\nle = LabelEncoder()\\ndf_train[\\'Subject\\'] = le.fit_transform(df_train[\\'Subject\\'])\\ndf_test[\\'Subject\\'] = le.fit_transform(df_test[\\'Subject\\'])\\n\\ndf_train = df_train.fillna(0)\\ndf_test = df_test.fillna(0)\\n\\n# split the dataset into training and testing sets\\ny_train = df_train[\\'Sentiment\\']\\nX_train = df_train.drop(\\'Sentiment\\', axis = 1)\\ny_test = df_test[\\'Sentiment\\']\\nX_test = df_test.drop(\\'Sentiment\\', axis = 1)\\n\\nX_train_tok = X_train.to_numpy()\\nX_test_tok = X_test.to_numpy()\\n\\nmax_seq_len = 500\\n\\n# clip sequences\\nX_train_tok = [s[:max_seq_len] for s in X_train_tok]\\nX_test_tok = [s[:max_seq_len] for s in X_test_tok]\\n\\n# create masks\\nmask_train = [[1]*len(s)+[0]*(max_seq_len-len(s)) for s in X_train_tok]\\nmask_test = [[1]*len(s)+[0]*(max_seq_len-len(s)) for s in X_test_tok]\\n\\n# pad sequences\\n# X_train_tok = [s+[0]*(max_seq_len-len(s)) for s in X_train_tok]\\n# X_test_tok = [s+[0]*(max_seq_len-len(s)) for s in X_test_tok]\\n\\ntrain_dset = torch.utils.data.TensorDataset(torch.tensor(X_train_tok, \\n                                                         dtype=torch.long), \\n                                            torch.tensor(y_train, \\n                                                         dtype=torch.long), \\n                                            torch.tensor(mask_train, \\n                                                         dtype=torch.float))\\n\\ntest_dset = torch.utils.data.TensorDataset(torch.tensor(X_test_tok, \\n                                                        dtype=torch.long), \\n                                          torch.tensor(y_test, \\n                                                        dtype=torch.long), \\n                                          torch.tensor(mask_test, \\n                                                        dtype=torch.float))\\n\\nbatch_size = 128\\n\\ntrain_loader = torch.utils.data.DataLoader(train_dset,\\n                          batch_size=batch_size,\\n                          shuffle=True,\\n                          num_workers=2\\n                         )\\n\\ntest_loader = torch.utils.data.DataLoader(test_dset,\\n                          batch_size=batch_size,\\n                          shuffle=False,\\n                          num_workers=2\\n                         )\\n\\n# demo\\nx, y, m = next(iter(train_loader))\\nprint(x.shape, y.shape, m.shape)\\n# x.shape: [B, L], y.shape: [L], mask.shape: [B, L]\\nprint(x)\\nprint(y)\\nprint(m)\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\nclass RNNClassifier(nn.Module):\\n    def __init__(self, embedding_dim, hidden_dim, vocab_size,                  num_rec_layers=1, rec_layer=nn.LSTM):\\n        super(RNNClassifier, self).__init__()\\n        # define all layers we need, \\n        # their parameters will be initialized automatically\\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\\n        self.rnn1 = rec_layer(embedding_dim, hidden_dim, batch_first=True)\\n        self.num_rec_layers = num_rec_layers\\n        if self.num_rec_layers == 2:\\n            self.rnn2 = rec_layer(hidden_dim, hidden_dim, batch_first=True)\\n            # this code with if is for simplicity, \\n            # normally one would use torch.nn.ModuleList\\n        self.hidden2label = nn.Linear(hidden_dim, 1)\\n    \\n    def forward(self, sentences, mask):\\n        # sentences shape: [B, L], mask shape: [B, L]\\n        embedding = self.word_embeddings(sentences) # shape: [B, L, DE]\\n        out, hidden = self.rnn1(embedding) # out shape: [B, L, DH]\\n        if self.num_rec_layers == 2:\\n            out, hidden = self.rnn2(out) # out shape: [B, L, DH]\\n        out = (out*mask[:, :, None]).mean(dim=1) # shape: [B, DH]\\n        res = self.hidden2label(out) # shape: [B, 1]\\n        return torch.sigmoid(res)\\n\\n# create a particular instance of the model \\nrnn = RNNClassifier(128, 128, tokenizer.vocab_size)\\nrnn.to(device)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roberta"
      ],
      "metadata": {
        "id": "qtEuBFDvh82p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roberta_preprocess(data):\n",
        "    #renaming the columns \n",
        "    data = data.rename(columns={0:'Sentiment',1:'Category',2:'Subject',3:'Index',4:'Text'})\n",
        "\n",
        "    # Lowercase\n",
        "    data[\"Text\"] = data[\"Text\"].str.lower()\n",
        "    data[\"Subject\"] = data[\"Subject\"].str.lower()\n",
        "\n",
        "    # Special Token\n",
        "    data[['Start','End']] = data['Index'].str.split(':', 1, expand=True)\n",
        "    data['Start'] = data['Start'].astype(int)\n",
        "    data['End'] = data['End'].astype(int)\n",
        "    data['Text'] = data.apply(lambda x: x['Text'][:x['Start']]+'\\\"'+x['Text'][x['Start']:x['End']]+'\\\"'+x['Text'][x['End']:],axis=1)\n",
        "\n",
        "    data = data.drop(columns=['Index','Start','End'])\n",
        "\n",
        "    # Separating 2 columns\n",
        "    data['Category'] = data['Category'].str.lower()\n",
        "    data[['Main_Category','Sub_Category']] = data['Category'].str.split('#', 1, expand=True)\n",
        "    data['Sub_Category'] = data['Sub_Category'].str.replace('_',' ')\n",
        "    data['Category'] = data['Main_Category'] + ' ' + data['Sub_Category']\n",
        "\n",
        "    data['Text'] = data['Text'] + ' <s> ' + data['Category'] + ' </s>'\n",
        "    #data = data.drop(columns='Category')\n",
        "\n",
        "    # Label Encoding Sentiment, Category, and Sub-Category\n",
        "    data['Sentiment'] = data['Sentiment'].apply(lambda x: 2 if x == 'positive' else (1 if x == 'neutral' else 0))\n",
        "    #data['Main_Category_Label'] = data['Main_Category'].apply(lambda x: 1 if x == 'AMBIENCE' else (2 if x == 'FOOD' else (3 if x == 'SERVICE' else (4 if x == 'RESTAURANT' else (5 if x == 'DRINKS' else 6)))))\n",
        "    #data['Sub_Category_Label'] = data['Sub_Category'].apply(lambda x: 1 if x == 'GENERAL' else (2 if x == 'QUALITY' else (3 if x == 'STYLE_OPTIONS' else (4 if x == 'MISCELLANEOUS' else 5))))\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "H5yzKp1q_Q7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "train = pd.read_csv('traindata.csv',sep='\t',header=None)\n",
        "\n",
        "train = roberta_preprocess(train)\n",
        "\n",
        "test = pd.read_csv('devdata.csv',sep='\t',header=None)\n",
        "\n",
        "test = roberta_preprocess(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzczFu0x_Ez7",
        "outputId": "dac4b833-5d52-4b83-de91-5b5ce2a8728d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-7d4681bcbeef>:10: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
            "  data[['Start','End']] = data['Index'].str.split(':', 1, expand=True)\n",
            "<ipython-input-2-7d4681bcbeef>:19: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
            "  data[['Main_Category','Sub_Category']] = data['Category'].str.split('#', 1, expand=True)\n",
            "<ipython-input-2-7d4681bcbeef>:10: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
            "  data[['Start','End']] = data['Index'].str.split(':', 1, expand=True)\n",
            "<ipython-input-2-7d4681bcbeef>:19: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
            "  data[['Main_Category','Sub_Category']] = data['Category'].str.split('#', 1, expand=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "train['Input'] = train['Text'].apply(lambda x: tokenizer(x, padding='max_length', max_length=100)['input_ids'])\n",
        "train['Mask'] = train['Text'].apply(lambda x: tokenizer(x, padding='max_length', max_length=100)['attention_mask'])\n",
        "\n",
        "test['Input'] = test['Text'].apply(lambda x: tokenizer(x, padding='max_length', max_length=100)['input_ids'])\n",
        "test['Mask'] = test['Text'].apply(lambda x: tokenizer(x, padding='max_length', max_length=100)['attention_mask'])"
      ],
      "metadata": {
        "id": "PUkWbYCUJPIq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train[['Input','Mask']]\n",
        "y = np.array(train['Sentiment'].tolist())\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_test = test[['Input','Mask']]\n",
        "y_test = np.array(test['Sentiment'].tolist())"
      ],
      "metadata": {
        "id": "NVmyNbaIJ2I7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(X_train['Input'].tolist()), dtype=torch.long),\n",
        "                                               torch.tensor(np.array(X_train['Mask'].tolist()), dtype=torch.long),\n",
        "                                               torch.tensor(y_train, dtype=torch.long))\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(X_val['Input'].tolist()), dtype=torch.long),\n",
        "                                             torch.tensor(np.array(X_val['Mask'].tolist()), dtype=torch.long),\n",
        "                                             torch.tensor(y_val, dtype=torch.long))\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(np.array(X_test['Input'].tolist()), dtype=torch.long),\n",
        "                                              torch.tensor(np.array(X_test['Mask'].tolist()), dtype=torch.long),\n",
        "                                              torch.tensor(y_test, dtype=torch.long))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True,num_workers=2)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=32,shuffle=False,num_workers=2)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=2)"
      ],
      "metadata": {
        "id": "2khWj0HAM-PH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "Roberta = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
        "'''\n",
        "for param in Roberta.parameters():\n",
        "    param.requires_grad = False\n",
        "'''\n",
        "class model(nn.Module):\n",
        "\n",
        "    def __init__(self, Roberta):\n",
        "        super().__init__()\n",
        "\n",
        "        self.roberta = Roberta.to(device)\n",
        "        self.l1 = nn.Linear(in_features=768, out_features=768)\n",
        "        #self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "        self.l2 = nn.Linear(in_features=768, out_features=3)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "\n",
        "        x = self.roberta(x,attention_mask)\n",
        "        x = x.pooler_output\n",
        "        x = self.l1(x)\n",
        "        #x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.l2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "basic_model = model(Roberta).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gpEBWUmazY9",
        "outputId": "1c5e668f-5b37-494f-f92d-8be18e11d5be"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
        "\n",
        "    epoch_list = []\n",
        "    scores_list = []\n",
        "    lowest_loss = 1\n",
        "\n",
        "    # loop over epochs\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        # loop over batches\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, mask, labels = data\n",
        "            outputs = model(inputs.to(device),mask.to(device))\n",
        "            # compute the loss\n",
        "            loss = loss_fcn(outputs, labels.to(device))\n",
        "            # optimizer step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            losses.append(loss.item())\n",
        "        loss_data = np.array(losses).mean()\n",
        "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch, loss_data))\n",
        "        \n",
        "        if epoch % 5 == 0:\n",
        "            # evaluate the model on the validation set\n",
        "            # computes the f1-score\n",
        "            score_list_batch = []\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for i, batch in enumerate(val_dataloader):\n",
        "                    inputs, mask, labels = batch\n",
        "                    output = model(inputs.to(device),mask.to(device))\n",
        "                    loss_test = loss_fcn(output, labels.to(device))\n",
        "                    predict = torch.argmax(output,axis=1)\n",
        "                    score = accuracy_score(labels.cpu().numpy(), predict.cpu().numpy())\n",
        "                    score_list_batch.append(score)\n",
        "\n",
        "            score = np.array(score_list_batch).mean()\n",
        "            print(\"Accuracy-Score: {:.4f}\".format(score))\n",
        "            scores_list.append(score)\n",
        "            epoch_list.append(epoch)\n",
        "\n",
        "    return epoch_list, scores_list"
      ],
      "metadata": {
        "id": "2tyoOBagL4jb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Max number of epochs\n",
        "max_epochs = 41\n",
        "\n",
        "### DEFINE LOSS FUNCTION\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "loss_fcn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "### DEFINE OPTIMIZER\n",
        "optimizer = torch.optim.AdamW(basic_model.parameters(), lr=2e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,0,max_epochs*len(train_dataloader))\n",
        "epoch_list, basic_model_scores = train(basic_model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "-TCRbjNT3rJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1936b5a-f5bf-4e06-a7cb-f78b5795d376"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00000 | Loss: 1.1003\n",
            "Accuracy-Score: 0.7252\n",
            "Epoch 00001 | Loss: 1.0958\n",
            "Epoch 00002 | Loss: 0.8633\n",
            "Epoch 00003 | Loss: 0.8291\n",
            "Epoch 00004 | Loss: 0.6310\n",
            "Epoch 00005 | Loss: 0.4135\n",
            "Accuracy-Score: 0.8079\n",
            "Epoch 00006 | Loss: 0.2854\n",
            "Epoch 00007 | Loss: 0.2172\n",
            "Epoch 00008 | Loss: 0.1588\n",
            "Epoch 00009 | Loss: 0.1340\n",
            "Epoch 00010 | Loss: 0.1104\n",
            "Accuracy-Score: 0.8829\n",
            "Epoch 00011 | Loss: 0.1073\n",
            "Epoch 00012 | Loss: 0.1141\n",
            "Epoch 00013 | Loss: 0.1426\n",
            "Epoch 00014 | Loss: 0.1028\n",
            "Epoch 00015 | Loss: 0.0713\n",
            "Accuracy-Score: 0.8923\n",
            "Epoch 00016 | Loss: 0.0533\n",
            "Epoch 00017 | Loss: 0.0613\n",
            "Epoch 00018 | Loss: 0.0708\n",
            "Epoch 00019 | Loss: 0.0943\n",
            "Epoch 00020 | Loss: 0.0599\n",
            "Accuracy-Score: 0.8829\n",
            "Epoch 00021 | Loss: 0.0377\n",
            "Epoch 00022 | Loss: 0.0323\n",
            "Epoch 00023 | Loss: 0.0228\n",
            "Epoch 00024 | Loss: 0.0180\n",
            "Epoch 00025 | Loss: 0.0147\n",
            "Accuracy-Score: 0.8986\n",
            "Epoch 00026 | Loss: 0.0141\n",
            "Epoch 00027 | Loss: 0.0219\n",
            "Epoch 00028 | Loss: 0.0146\n",
            "Epoch 00029 | Loss: 0.0164\n",
            "Epoch 00030 | Loss: 0.0263\n",
            "Accuracy-Score: 0.8954\n",
            "Epoch 00031 | Loss: 0.0125\n",
            "Epoch 00032 | Loss: 0.0073\n",
            "Epoch 00033 | Loss: 0.0114\n",
            "Epoch 00034 | Loss: 0.0117\n",
            "Epoch 00035 | Loss: 0.0073\n",
            "Accuracy-Score: 0.8954\n",
            "Epoch 00036 | Loss: 0.0056\n",
            "Epoch 00037 | Loss: 0.0058\n",
            "Epoch 00038 | Loss: 0.0063\n",
            "Epoch 00039 | Loss: 0.0059\n",
            "Epoch 00040 | Loss: 0.0130\n",
            "Accuracy-Score: 0.8892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### F1-SCORE ON TEST DATASET\n",
        "score_list_batch = []\n",
        "\n",
        "basic_model.eval()\n",
        "\n",
        "for i, batch in enumerate(test_dataloader):\n",
        "    inputs, mask, labels = batch\n",
        "    output = basic_model(inputs.to(device),mask.to(device))\n",
        "    loss_test = loss_fcn(output, labels.to(device))\n",
        "    predict = torch.argmax(output,axis=1)\n",
        "    score = accuracy_score(labels.cpu().numpy(), predict.cpu().numpy())\n",
        "    score_list_batch.append(score)\n",
        "\n",
        "score = np.array(score_list_batch).mean()\n",
        "print(\"Basic Model : Accuracy Score on the test set: {:.4f}\".format(score))\n",
        "\n",
        "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
        "def plot_score(epoch_list, scores) :\n",
        "    plt.figure(figsize=[10,5])\n",
        "    plt.plot(epoch_list, scores)\n",
        "    plt.title(\"Evolution of Score w.r.t epochs\")\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.show()\n",
        "    \n",
        "plot_score(epoch_list, basic_model_scores)"
      ],
      "metadata": {
        "id": "4TovSdniMei7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "9a1387df-dbde-48ba-fa66-91fed0ac7279"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Model : Accuracy Score on the test set: 0.8793\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F1OLKTXkPnH",
        "outputId": "000dad7b-69bd-4bc0-9c43-9a054e918400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({2: 264, 0: 98, 1: 14})"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(basic_model.state_dict(), 'best_model.pt')"
      ],
      "metadata": {
        "id": "Nr6eQjIb9hOq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_model"
      ],
      "metadata": {
        "id": "cM5CBvIEIAoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}